{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on I :  White Box Techniques ( ~ 30 minutes) \n",
    "\n",
    "Guideline:  \n",
    " \n",
    "- Prerequisites: Data Preparation (Understand and load the data) \n",
    "- Decision Rules\n",
    "    - Building decision rules\n",
    "    - OneR \n",
    "    - ZeroR \n",
    "- Decision Trees \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries:  Understand the problem and load the data\n",
    "This dataset contains information about survival passengers of titanic. The objective of the task is to classify if a passenger survives or not according to the features. \n",
    "\n",
    "The dataset is available in folder: data/titanic.xls\n",
    "\n",
    "Attribute Information (in order):\n",
    "\n",
    "    - pclass     Passenger class \n",
    "    - name       Name\n",
    "    - sex        Sex \n",
    "    - age        Age\n",
    "    - sibsp      Number of Siblings/Spouses Aboard\n",
    "    - parch      Number of Parents/Children Aboard\n",
    "    - ticket     Ticket Number\n",
    "    - fare       Passenger Fare\n",
    "    - cabin      Cabin \n",
    "    - embarked   Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "    - boat       Lifeboat (if survived)\n",
    "\n",
    "Target Variable \n",
    "    - survided     The passenger survived or not \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def print_metrics(y_test: list, y_pred: list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_test (list):  Ground truth (correct) labels.\n",
    "\n",
    "        y_pred (list): Predicted labels, as returned by a classifier.\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(\"Recall  : {}\".format(recall_score(y_test, y_pred)))\n",
    "    print(\"F1-Score: {}\".format(f1_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "#Read Data from source \n",
    "data_df = pd.read_excel(\"Data/titanic_dataset.xls\").set_index(\"name\")\n",
    "\n",
    "# Split the data into train and test set \n",
    "x_train, x_test, y_train, y_test = train_test_split(data_df.iloc[:, :-1], data_df[\"survived\"], \n",
    "                                                    test_size=0.25, random_state=4242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Rules\n",
    "Decision rules are a set of IF-THEN rules. The combination of several rules can be used to make predictions. The rules can be defined manually by the user (if the user has good domain knowledge it can encode very good rules). \n",
    "\n",
    "An IF-THEN rule is an expression of the form: \n",
    "```\n",
    "IF condition THEN conclusion\n",
    "\n",
    "# Example: \n",
    "IF age=youth AND love_coffe=YES AND favourite_meetup=\"Python Barcelona\"  \n",
    "        THEN uses_python=YES\n",
    "ELSE uses_python=NO\n",
    "\n",
    "```\n",
    "In this section is proposed to think and build your own decision rules set (just making some hypothesis about the dataset). \n",
    "\n",
    "\n",
    "### EXAMPLE\n",
    "A) Analyze the dataset and encode an IF-ELSE block   \n",
    "        \n",
    "B) Build y_pred (an array with the binary probability class of the input samples). Use the test set\n",
    "        1 if the passenger survived \n",
    "        0 if not survived \n",
    "\n",
    "C) Evaluate using print_metrics() funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the dataset and encode an IF-ELSE block \n",
    "x_train_dr = x_train.copy()\n",
    "\n",
    "# IF EMBARKED = Q THEN survived = YES \n",
    "# ELSE survived = NO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build y_pred using previous rule \n",
    "x_test_dr = x_test.copy()\n",
    "\n",
    "x_test_dr[\"index\"] = range(0, x_test_dr.shape[0]) \n",
    "x_test_dr = x_test_dr[x_test_dr[\"embarked\"] == \"Q\"]\n",
    "\n",
    "y_pred = np.zeros(y_test.shape[0])\n",
    "np.put(y_pred, x_test_dr[\"index\"].tolist(), np.ones(y_test.shape[0], dtype = int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5640243902439024\n",
      "Recall  : 0.07751937984496124\n",
      "F1-Score: 0.12269938650306748\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the rule\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "Follow the previous steps and encode your own rule \n",
    "\n",
    "A) Analyze the dataset and encode an IF-ELSE block   \n",
    "        \n",
    "B) Build y_pred (an array with the binary probability class of the input samples). Use the test set\n",
    "        1 if the passenger survived \n",
    "        0 if not survived \n",
    "\n",
    "C) Evaluate using print_metrics() funtion (using y_pred and y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneR Algorithm\n",
    "OneR probably is one of the simplest methods for classification (for discrete attributes)  due to the simplicity we can quickly explain each prediction.  Although the simplicity of this algorithm it is only a few percentage points less accurate than decision trees (source: Very Simple Classification Rules Perform Well on Most Commonly Used Datasets [link](https://www.mlpack.org/papers/ds.pdf)) \n",
    "\n",
    "OneR works as follows: \n",
    "\n",
    "```\n",
    "For feature in the dataset: \n",
    "     We build a Frequency table \n",
    "         - 1. Count how often each value of target appears in category groups\n",
    "         - 2. Encode the frequency class into a rule\n",
    "         - 3. Calculate the quality of the rule \n",
    "\n",
    "The best predictor is chosen as the one with the smallest error\n",
    "```\n",
    "Source OneR : [link](https://www.saedsayad.com/oner.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embarked  survived\n",
       "C         0           100\n",
       "          1           108\n",
       "Q         0            55\n",
       "          1            36\n",
       "S         0           455\n",
       "          1           227\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Count how often each value of target appears \n",
    "# 2. Find the most frequent class \n",
    "pd.concat([x_train, pd.Series(y_train, name=\"survived\")], axis = 1) \\\n",
    "  .groupby(['embarked','survived']) \\\n",
    "  .size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Make the rule assigning that class to this value\n",
    "# Using the previous frequency we can determine the following rules: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "IF embarked = C THEN survived = YES\n",
    "IF embarked = Q THEN survived = NO\n",
    "IF embarked = S THEN survived = NO\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6737804878048781\n",
      "Recall  : 0.32558139534883723\n",
      "F1-Score: 0.4397905759162304\n"
     ]
    }
   ],
   "source": [
    "# Codify previous rules\n",
    "rule_codification = {\"C\": 1, \"Q\": 0, \"S\": 0}\n",
    "y_pred = x_test['embarked'].map(rule_codification)\n",
    "\n",
    "# Evaluate the rule\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO \n",
    "\n",
    "A) Build a Frequency table for sex feature \n",
    "\n",
    "B) Encode the Frequency table to a rule  \n",
    "\n",
    "C) Compare the embarked predictor with sex predictor, better or worst? \n",
    "\n",
    "D) Implement ZeroR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:  Frequency table using sex feature \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B: Encode the Frequency table to a rule \n",
    "# hint consider using map function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C: Evaluate the rule  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D) Implement ZeroR Algorithm \n",
    "ZeroR algorithm is even more simple. This algorithm is based on predicting the majority class, the classifier relies only on the target value and ignores the predictors. \n",
    "\n",
    "Example: imagine a dataset for email spam classification (is_spam) looking at the target value we found 57 SPAM observations and 20 of NO SPAM observations. ZeroR builts the following rule: is_spam(X) = YES, in other words: for all predicted instances is returned YES. \n",
    "Of course, the limitations of ZeroR and OneR are obvious but these two algorithms can be used as a useful baseline for Machine Learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurences on test set (y_test) and determine mayority class (Survived or NOT). \n",
    "\n",
    "\n",
    "# Evaluate the rule using y_test print_metrics() funtion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "Decision Trees is one of the top popular supervised Machine Learning methods, it builds a classifier or a regressor model in the form of a tree structure. \n",
    "This algorithm is simple to understand and interpret;  we can easily print the decision tree or determine the decision path of a prediction.  \n",
    "\n",
    "\n",
    "<div>\n",
    "    <center> <img src=\"Data/img/decision_tree.JPG\" width=\"500\" /> </center>\n",
    "</div>\n",
    "\n",
    "During this section is show how to output graphical trees (using Graphviz, a Graph visualization Software). Last section includes a function to print the decision path.\n",
    "\n",
    "decision_path() is a function from DecisionTreeClassifier which returns a sparse matric showing which nodes of the tree the prediction goes through,  this information can be used to understand the why of a prediction. \n",
    "\n",
    "### TODO\n",
    "\n",
    "A) Preprocess data. Detect Missing values and encode categorical features using OneHotEncoder. Use the function preprocessing_dataframe()\n",
    "\n",
    "B) Train a decision tree model using x_train and x_test\n",
    "\n",
    "C) Evaluate the model using print_metrics() funtion\n",
    "\n",
    "D) Print the tree using graphviz \n",
    "\n",
    "E) Determine the decision path for an observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "def preprocessing_dataframe(data_df: object, missing_values_to_convert : list, categorical_to_encode: list) -> object: \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset (Dataframe): Dataset \n",
    "\n",
    "        missing_values_to_convert (list): List of features names from dataset to impute missing values. Each missing value is \n",
    "            fill with mean\n",
    "        \n",
    "        categorical_to_encode (list): List of categorical features names to encode using OneHotEncoding\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: converted dataset\n",
    "    \"\"\"\n",
    "        \n",
    "    # Handle Missing Values\n",
    "    for feature in missing_values_to_convert: \n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        data_df[feature] = imputer.fit_transform(data_df[[feature]])\n",
    "\n",
    "    # One Hot Encoding\n",
    "    for feature in categorical_to_encode: \n",
    "        onehot_encoding = pd.get_dummies(data_df[feature],prefix=feature)\n",
    "        data_df = pd.concat([data_df, pd.get_dummies(data_df[feature], prefix=feature)],axis=1)\n",
    "        data_df.drop([feature],axis=1, inplace=True)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "# Preprocessing, detect missing vaules and encode categorical features \n",
    "# use the function to preprocessing_dataframe() on x_train and x_test\n",
    "x_train_dtree = preprocessing_dataframe(x_train.copy(), [\"age\", \"fare\"], [\"sex\", \"embarked\"])\n",
    "x_test_dtree = preprocessing_dataframe(x_test.copy(), [\"age\", \"fare\"], [\"sex\", \"embarked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Decision Tree model (use the implementation from scikit-learn library) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using metrics funtion \n",
    "y_pred = \n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "def visualize_tree(model: object, feature_names: list) -> object:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (object): Decision Tree model  \n",
    "\n",
    "        feature (list): List of features names \n",
    "        \n",
    "    Returns:\n",
    "        Image: graphical decision tree image\n",
    "    \"\"\"\n",
    "\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(model, out_file=dot_data, feature_names = feature_names,\n",
    "                    filled=True, rounded=True, special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    return Image(graph.create_png())\n",
    "\n",
    "# Use the function visualize_tree to print the tree structure\n",
    "# Control de complexity of the three using max_depth\n",
    "visualize_tree(  ,  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decision_path(dtree: object, dataset: object, sample_id: int): \n",
    "    \"\"\"\n",
    "    Info: Adapted from https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html \n",
    "    \n",
    "    Args:\n",
    "        model (object): Decision Tree model  \n",
    "\n",
    "        dataset (DataFrame): Dataset\n",
    "        \n",
    "        sample_id (int) : row identifier \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    node_indicator = dtree.decision_path(dataset)\n",
    "    feature = dtree.tree_.feature\n",
    "    feature_names = x_test_dtree.columns.values\n",
    "    leave_id = dtree.apply(dataset)\n",
    "\n",
    "    node_index = node_indicator.indices[node_indicator.indptr[sample_id]: node_indicator.indptr[sample_id + 1]]\n",
    "    threshold = dtree.tree_.threshold\n",
    "    print(\"Decision Path for sample : {} (predicted as {}) \\n\".format(sample_id, dtree.predict([x_test_dtree.iloc[sample_id]])[0]))\n",
    "    for node_id in node_index:\n",
    "        if leave_id[sample_id] == node_id:\n",
    "            continue\n",
    "            \n",
    "        if x_test_dtree.iloc[sample_id, feature[node_id]] <= threshold[node_id]:\n",
    "            threshold_sign = \"<=\"\n",
    "        else:\n",
    "            threshold_sign = \">\"\n",
    "\n",
    "        print(\"decision id node {} : {} (= {}) {} {})\".format(node_id, feature_names[feature[node_id]], \n",
    "                                                              x_test_dtree.iloc[sample_id, feature[node_id]], threshold_sign, threshold[node_id]))\n",
    "\n",
    "        \n",
    "# Use the function print_decision_path to obtain the decision path for an instance (sample_id) in x_test (dataset)\n",
    "print_decision_path( ,  , )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
