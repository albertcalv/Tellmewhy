{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on II :  Black Box Techniques ( ~ 30 minutes) \n",
    "\n",
    "Guideline:  \n",
    " \n",
    "- Prerequisites  : Data Preparation (Understand and load the data) \n",
    "- Train a ML models (black box and white box models)\n",
    "- LIME  \n",
    "- SHAP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites: Data Preparation\n",
    "\n",
    "During the hands-on is proposed to use boston dataset from sklearn_datasets utilities  \n",
    "\n",
    "This dataset contains information about boston house prices. The objective of the task is to predict the value of a house.  (see : [sklearn.datasets.load_boston.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn-datasets-load-boston) ) \n",
    "\n",
    "Attribute Information (in order):\n",
    "\n",
    "    - CRIM     per capita crime rate by town\n",
    "    - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    - INDUS    proportion of non-retail business acres per town\n",
    "    - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "    - NOX      nitric oxides concentration (parts per 10 million)\n",
    "    - RM       average number of rooms per dwelling\n",
    "    - AGE      proportion of owner-occupied units built prior to 1940\n",
    "    - DIS      weighted distances to five Boston employment centres\n",
    "    - RAD      index of accessibility to radial highways\n",
    "    - TAX      full-value property-tax rate per \\$10,000\n",
    "    - PTRATIO  pupil-teacher ratio by town\n",
    "    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "    - LSTAT    % lower status of the population\n",
    "Target Variable \n",
    "    - MEDV     Median value of owner-occupied homes in $1000's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Dataset\n",
    "boston = load_boston()\n",
    "boston_data_df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "# Train and test split \n",
    "x_train, x_test, y_train, y_test = train_test_split(boston_data_df, boston.target, \n",
    "                                                    test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites  : Train different ML models \n",
    "Build a High Accuracy and a High Explainability ML model.\n",
    "\n",
    "\n",
    "<div>\n",
    "    <center> <img src=\"Data/img/accuracy_explainability_tradeoff.jpg\" width=\"350\"/> </center>\n",
    "</div>\n",
    "\n",
    "### TODO\n",
    "\n",
    "1. Train different models using previous dataset. Train at least two different models: \n",
    "        - Model a: High Accuracy model  \n",
    "        - Model b: High Explainaiblity model  \n",
    "2. Calculate and compare the performance of the models \n",
    "\n",
    "__note__ For simplicity consider only working with models included in scikit library. \n",
    "\n",
    "__note__ We are working with Regression choose a good model/metric for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model a : High Explainability algorithm  (Suggested DecisionTree from sci-kit library)\n",
    "\n",
    "# Define model b : High Accuracy algorithm (Suggested GradienBoosting from sci-kit library)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST-HOC Explainability  \n",
    "Post-Hoc techniques allows to train any Machine Learning model (model agnostic) as usual and provide explanations a posteriori. This technique tends to be more flexible and independent in comparison with WhiteBox techniques where the level of explainability is constrained by the nature of the algorithm.   \n",
    "\n",
    "<div>\n",
    "    <center> <img src=\"Data/img/posthoc_explanation.JPG\" width=\"350\"/> </center>\n",
    "</div>\n",
    "\n",
    "\n",
    "## LIME \n",
    "LIME is based on surrogate models. Surrogate models are used when the outcome of interest cannot be directly measured. Specifically,  LIME creates for each instance a local model to provide an explanation. For each point in the dataset, the surrogate model makes a perturbation (for instance, adding noise to continuous features, removing words or hiding parts of the image) and evaluates the changes in the output. \n",
    "\n",
    "<div>\n",
    "    <center> <img src=\"Data/img/lime_explanation.png\" width=\"400\"/> </center>\n",
    "</div>\n",
    "\n",
    "The figure above illustrates the intuition for this procedure. The model decision function is represented by the blue/pink background. The bright red cross is the instance being explained (let's call it X). We sample instances around X, and weight them according to their proximity to X (weight here is indicated by size). We then learn a linear model (dashed line) that approximates the model well in the vicinity of X, but not necessarily (source: https://github.com/marcotcr/lime) \n",
    "\n",
    "### How to interpret a LIME instance: \n",
    "The output of LIME is a list of features reflecting the contributions of each feature in the predicted observation. It works as follows, Imagine a classifier for Mushroom poisoning (Poisonous or Edible). Looking at the output, the predicted probability told us that the classifier predicts \"poisonous\".  The bar plot told which features are pushing the prediction as poisonous. The odor=foul, stalk-surface-above-ring=silky and spore-print-color=choloate are the features that contribute to building the observation as poisonous. \n",
    "\n",
    "<div>\n",
    "    <center> <img src=\"Data/img/lime_example.png\" width=\"600\"/> </center>\n",
    "</div>\n",
    "\n",
    "\n",
    "### TODO\n",
    "A) Define a LimeTabularExplainer for your dataset (using LimeTabularExplainer)\n",
    "\n",
    "B) Explain an instance of the test set (using explain_instance function) \n",
    "\n",
    "C) Explain the instance with the lowest predicted value \n",
    "\n",
    "D) Explain the instance with the highest predicted value \n",
    "\n",
    "E) Compare both explanations\n",
    "\n",
    "\n",
    "**LimeTabularExplainer** object contain functions to explain predictions on tabular data ([link](https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=explainer%20explain_instance#lime.lime_tabular.LimeTabularExplainer.explain_instance)) \n",
    "\n",
    "![Explain instance](Data/img/lime_tabular_explainer.JPG)\n",
    "\n",
    "Expected Parameters: \n",
    "\n",
    "- training_data – numpy 2d array\n",
    "- mode – “classification” or “regression”\n",
    "- feature_names – list of names (strings) corresponding to the columns in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Define a LimeTabularExplainer object for train data\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(training_data=,\n",
    "                                                   mode='regression',\n",
    "                                                   feature_names=  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the funtion **explain_instance** to generate an explanation for a single point (2d numpy array correspoing to a row).\n",
    "\n",
    "Expected Parameters: \n",
    "- data_row – 1d numpy array, corresponding to a row\n",
    "- predict_fn – prediction function. \n",
    "    For classifiers, this should be a function that takes a numpy array and outputs prediction probabilities. For regressors, this takes a numpy array and returns the\n",
    "    predictions. For ScikitClassifiers, this is classifier.predict_proba(). For ScikitRegressors, this is regressor.predict().\n",
    "- num_features - int, maximum number of features present in explanation\n",
    "\n",
    "<img src=\"Data/img/explain_instance.JPG\" width=\"700\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a random instance of the test set\n",
    "idx_instance = 15\n",
    "exp = explainer.explain_instance(data_row = , \n",
    "                                 predict_fn = , \n",
    "                                 num_features= )\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the instance with higher predicted value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the instance with lowest predicted value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP \n",
    "SHAP (SHapley Additive exPlanations) explain instances using Shapley values a game theory method. Shapley value is a solution concept in cooperative game theory. To each cooperative game it assigns a unique distribution (among the players) of a total surplus generated by the coalition of all players.  \n",
    "\n",
    "The Shapley value is one way to distribute the total gains to the players, assuming that they all collaborate. It is a \"fair\" distribution in the sense that it is the only distribution with certain desirable properties listed below. According to the Shapley value, the amount that player i gets given in a coalitional game (v,N).  N is the total number of players and the sum extends over all subsets S of N not containing player (Source: [link](https://en.wikipedia.org/wiki/Shapley_value)) SHAP explain each prediction (point in the dataset) by computing the contribution of each feature to the prediction. For each point is presented a consistent and locally feature atributon based on expectations.\n",
    "\n",
    "\n",
    "<div>\n",
    "    <center> <img src=\"Data/img/shapley_values.svg\" width=\"500\"/> </center>\n",
    "</div>\n",
    "\n",
    "\n",
    "Read more : [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) (probably explained better than here)\n",
    "\n",
    "\n",
    "### How to interpret SHAP Values\n",
    "Shap force plot shows the contributions of each feature to the final output. Features pushing the prediction higher are shown in red, pushing the prediction lower are in blue. In this case, the model ouput is 24.41 and the average value 22.34 so this prediction is higher than the average values. A high LSTAT and a low RM and NOX contributes to make the prediction. \n",
    "\n",
    "<div>\n",
    "    <center> <img src=\"Data/img/boston_instance.png\" width=\"600\"/> </center>\n",
    "</div>\n",
    "\n",
    "\n",
    "### How to interpret a SHAP Summary plots: \n",
    "The summary plot gives an overview of which features are the most important in the prediction. In the plot are represented the SHAP values showing the distribution of the impacts.\n",
    "Each dot has three characteristics:\n",
    "- Vertical location shows what feature it is representing\n",
    "- Color shows whether that feature was high or low for that row of the dataset (red = high, blue = low)\n",
    "- Horizontal location shows whether the effect of that value caused a higher or lower prediction\n",
    "\n",
    "\n",
    "<div>\n",
    "    <center> <img src=\"Data/img/boston_summary_plot.png\" width=\"300\"/> </center>\n",
    "</div>\n",
    "\n",
    "A high level of LSTAT (% lower status of the population) lowers the predicted home price. On the other hand, a high level of RM (Rooms per Dwelling, Room of the house)  indicates a high level in home price.\n",
    "\n",
    "### How to interpret SHAP Dependence Contributions plots \n",
    "SHAP contribution plot shows the distributions of effects. In this case coloring RAD (index of accessibility to radial highways) highlights that the average number of rooms per house (RM) has less impact on home price for areas with a high RAD value.\n",
    "<div>\n",
    "    <center> <img src=\"Data/img/boston_dependence_plot.png\" width=\"300\"/> </center>\n",
    "</div>\n",
    "\n",
    "\n",
    "SHAP Documentation: [shap.readthedocs.io](https://shap.readthedocs.io/en/latest/)\n",
    "\n",
    "### TODO\n",
    "\n",
    "A) Build an Explainer object (KernelExplainer or TreeExplainer) and obtain shap values using the HighAccuracy algorithm using the test set \n",
    "\n",
    "B) Explain a Random Instance from test \n",
    "\n",
    "C) Explain the instance with higher predicted value \n",
    "\n",
    "D) Explain the instance with lower predicted value \n",
    "\n",
    "F) Summary Plot\n",
    "\n",
    "G) Dependence plot\n",
    "\n",
    "H) Repeat the previous steps (A-G) with the low accuracy algorithm, can you spot the diference in the explanations ? \n",
    "\n",
    "\n",
    "Kernel SHAP is a method that uses a special weighted linear regression and computes the importance of each feature. TreeExplainer is an optimization for Tree models.  Use the method shap_values to estimate the SHAP values for a set of points (This function returns a matrix indicating for each observation the set of weights). \n",
    "\n",
    "Sections B to G explain different function to plot and interpret SHAP values previously calculated.\n",
    "\n",
    "<img src=\"Data/img/shap_kernel.JPG\" width=\"600\"/> \n",
    "\n",
    "Expected parameters: \n",
    "- model :model object\n",
    "    The tree based machine learning model that we want to explain. XGBoost, LightGBM, CatBoost, and most tree-based scikit-learn models are supported.\n",
    "\n",
    "- data :numpy.array or pandas.DataFrame\n",
    "    The background dataset to use for integrating out features. This argument is optional when feature_dependence=”tree_path_dependent”, \n",
    "    since in that case we can use the number of training samples that went down each tree path as our background \n",
    "    dataset (this is recorded in the model object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Load JS Visualization code to notebook \n",
    "shap.initjs()\n",
    "\n",
    "# Consider use shap.kmeans(X, k) to summarize the x_train \n",
    "# X : Train Dataframe to summarize \n",
    "# k : Number of means to use for approximation \n",
    "\n",
    "# Use KernelExplainer for compute shap values of any model\n",
    "explainer_xgb = shap.KernelExplainer(model_xgb.predict, x_train)\n",
    "shap_values_model_xgb = explainer_xgb.shap_values(x_test,  l1_reg = \"num_features(10)\")\n",
    "\n",
    "# Use TreExplainer method if you use a Tree model (Decision Tree) \n",
    "explainer_dtr = shap.TreeExplainer(model_dtr)\n",
    "shap_values_model_dtr = explainer_dtr.shap_values(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function force_plot allows to visualize the SHAP values for a single instance  \n",
    "\n",
    "<img src=\"Data/img/shap_force_plot.JPG\" width=\"600\"/> \n",
    "\n",
    "Expected parameters: \n",
    "\n",
    "- base_value :float \n",
    "        This is the reference value that the feature contributions start from. For SHAP values it should be the value of explainer.expected_value.\n",
    "- shap_values :numpy.array\n",
    "         Matrix of SHAP values (# features) or (# samples x # features). If this is a 1D array then a single force plot will be drawn, if it is a 2D array  then a stacked force plot will be drawn.\n",
    "- features  :numpy.array\n",
    "        Matrix of feature values (# features) or (# samples x # features). This provides the values of all the features, and should be the same shape as the shap_values argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a random instance\n",
    "\n",
    "idx_instance = 15\n",
    "shap.force_plot(base_value = , \n",
    "                shap_values = , \n",
    "                features = ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the instance with lowest predicted value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the instance with higher predicted value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot also allows to explain multiple points in the plot. When the argument is a dataframe consisting in several observations \n",
    "# it will print the entire shap values rotating them 90 degres and stacking them horizontally. \n",
    "\n",
    "shap.force_plot(explainer_xgb.expected_value, shap_values_model_xgb, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function summary_plot summarizes all shap values. Use plot_type=\"bar\" to get a standard bar plot\n",
    "\n",
    "<img src=\"Data/img/shap_summary_plot.JPG\" width=\"600\"/> \n",
    "\n",
    "Expected Parameters: \n",
    "\n",
    "- shap_values : numpy.array\n",
    "        For single output explanations this is a matrix of SHAP values (# samples x # features).\n",
    "        For multi-output explanations this is a list of such matrices of SHAP values.\n",
    "- features : numpy.array or pandas.DataFrame or list\n",
    "        Matrix of feature values (# samples x # features) or a feature_names list as shorthand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function dependence plot print the interaction effects\n",
    "\n",
    "<img src=\"Data/img/shap_depence_plot.JPG\" width=\"600\"/> \n",
    "\n",
    "Expected Parameters: \n",
    "\n",
    "- ind :int or string \n",
    "        If this is an int it is the index of the feature to plot. If this is a string it is either the name of the feature to plot, or it can \n",
    "        have the form “rank(int)” to specify the feature with that rank (ordered by mean absolute SHAP value over all the samples)\n",
    "- shap_values :numpy.array \n",
    "        Matrix of SHAP values (# samples x # features).\n",
    "- features :numpy.array or pandas.DataFrame\n",
    "        Matrix of feature values (# samples x # features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Depence plot, the plot shows how the model depends on the given feature. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the previous steps (A-G) with the low accuracy algorithm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
